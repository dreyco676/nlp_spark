{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "# my local spark install\n",
    "findspark.init('/Users/dreyco676/spark-1.6.0-bin-hadoop2.6/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# create spark contexts\n",
    "sc = pyspark.SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "import preproc as pp\n",
    "# Register all the functions in Preproc with Spark Context\n",
    "check_lang_udf = udf(pp.check_lang, StringType())\n",
    "remove_stops_udf = udf(pp.remove_stops, StringType())\n",
    "remove_features_udf = udf(pp.remove_features, StringType())\n",
    "tag_and_remove_udf = udf(pp.tag_and_remove, StringType())\n",
    "lemmatize_udf = udf(pp.lemmatize, StringType())\n",
    "check_blanks_udf = udf(pp.check_blanks, StringType())\n",
    "numeric_label_udf = udf(pp.numeric_label, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a text file and convert each line to a Row.\n",
    "data_rdd = sc.textFile(\"data/raw_classified.txt\")\n",
    "parts_rdd = data_rdd.map(lambda l: l.split(\"\\t\"))\n",
    "# Filter bad rows out\n",
    "garantee_col_rdd = parts_rdd.filter(lambda l: len(l) == 3)\n",
    "# Create DataFrame\n",
    "data_df = sqlContext.createDataFrame(garantee_col_rdd, [\"text\", \"id\", \"text_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict language and filter out those with less than 90% chance of being English\n",
    "lang_df = data_df.withColumn(\"lang\", check_lang_udf(data_df[\"text\"]))\n",
    "en_df = lang_df.filter(lang_df[\"lang\"] == \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words to reduce dimensionality\n",
    "rm_stops_df = en_df.withColumn(\"stop_text\", remove_stops_udf(en_df[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove other non essential words, think of it as my personal stop word list\n",
    "rm_features_df = rm_stops_df.withColumn(\"feat_text\", remove_features_udf(rm_stops_df[\"stop_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag the words remaining and keep only Nouns, Verbs and Adjectives\n",
    "tagged_df = rm_features_df.withColumn(\"tagged_text\", tag_and_remove_udf(rm_features_df[\"feat_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization of remaining words to reduce dimensionality & boost measures\n",
    "lemm_df = tagged_df.withColumn(\"lemm_text\", lemmatize_udf(tagged_df[\"tagged_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all rows containing only blank spaces\n",
    "check_blanks_df = lemm_df.withColumn(\"is_blank\", check_blanks_udf(lemm_df[\"lemm_text\"]))\n",
    "no_blanks_df = check_blanks_df.filter(check_blanks_df[\"is_blank\"] == \"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_label_df = no_blanks_df.withColumn(\"label\", numeric_label_udf(no_blanks_df['text_label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "num_label_df.withColumnRenamed(num_label_df[\"lemm_text\"], \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dedupe important since alot of the tweets only differed by url's and RT mentions\n",
    "dedup_df = num_label_df.dropDuplicates(['text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the columns we care about\n",
    "data_set = dedup_df.select(num_label_df['id'], num_label_df['text'], num_label_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training & validation sets with 60% to training and use a seed value of 1987\n",
    "splits = data_set.randomSplit([0.6, 0.4])\n",
    "training_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "#\n",
    "#   Spark ML Section\n",
    "#   \n",
    "#   Skip Preprocessing and use cleaned files by running next cell\n",
    "#\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load already cleaned data\n",
    "def reload_checkpoint(data_rdd):\n",
    "    parts_rdd = data_rdd.map(lambda l: l.split(\"\\t\"))\n",
    "    # Filter bad rows out\n",
    "    garantee_col_rdd = parts_rdd.filter(lambda l: len(l) == 3)\n",
    "    typed_rdd = garantee_col_rdd.map(lambda p: (p[0], p[1], float(p[2])))\n",
    "    # Create DataFrame\n",
    "    df = sqlContext.createDataFrame(typed_rdd, [\"id\", \"text\", \"label\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load precleaned training set\n",
    "training_rdd = sc.textFile(\"data/clean_training.txt\")\n",
    "training_df = reload_checkpoint(training_rdd)\n",
    "# Load precleaned test set\n",
    "test_rdd = sc.textFile(\"data/clean_test.txt\")\n",
    "test_df = reload_checkpoint(test_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and nb.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "nb = NaiveBayes()\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, nb])\n",
    "\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 1.0]).build()\n",
    "\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, \n",
    "                    estimatorParamMaps=paramGrid, \n",
    "                    evaluator=MulticlassClassificationEvaluator(), \n",
    "                    numFolds=4)\n",
    "\n",
    "cvModel = cv.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cvModel.transform(test_df)\n",
    "prediction_df = result.select(\"text\", \"label\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n|                text|label|prediction|\n+--------------------+-----+----------+\n|acolyte warmachin...|  3.0|       2.0|\n|agree need keep m...|  3.0|       1.0|\n|alright entourage...|  3.0|       0.0|\n|alternative thank...|  3.0|       2.0|\n|annual spring cle...|  3.0|       2.0|\n|anyone aware acad...|  3.0|       2.0|\n|api script servic...|  1.0|       0.0|\n|artificialintelli...|  1.0|       1.0|\n|available skorne ...|  3.0|       2.0|\n|avoid fine penalt...|  3.0|       2.0|\n|barton armed scho...|  3.0|       2.0|\n|based learning bi...|  1.0|       1.0|\n|bedfordshire poli...|  3.0|       2.0|\n|beginning week ja...|  3.0|       0.0|\n|being new company...|  3.0|       2.0|\n|best part econ pa...|  3.0|       2.0|\n|best photo human ...|  3.0|       2.0|\n|better bulldog bu...|  3.0|       2.0|\n|better throw same...|  3.0|       2.0|\n|big data essentia...|  3.0|       1.0|\n+--------------------+-----+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "prediction_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n|                text|label|prediction|\n+--------------------+-----+----------+\n|multitun tunnel a...|  0.0|       0.0|\n|phdfootball scrip...|  0.0|       2.0|\n|python pic learn ...|  0.0|       2.0|\n|redis python modu...|  0.0|       0.0|\n|starterlearningpy...|  0.0|       2.0|\n|survival analysis...|  0.0|       1.0|\n|teleport json typ...|  0.0|       2.0|\n|command line tool...|  0.0|       0.0|\n|tckimlik personal...|  0.0|       0.0|\n|more tool data vi...|  0.0|       1.0|\n|python script tir...|  0.0|       2.0|\n|thirtylol lol src...|  0.0|       2.0|\n|good read impleme...|  0.0|       1.0|\n|html validator te...|  0.0|       2.0|\n|article tune regu...|  0.0|       0.0|\n|chainer dcgan cha...|  0.0|       2.0|\n|credstash utility...|  0.0|       2.0|\n|bot data analysis...|  0.0|       0.0|\n|data migration bl...|  0.0|       0.0|\n|great guide data ...|  0.0|       0.0|\n+--------------------+-----+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "datasci_df = prediction_df.filter(prediction_df['label']==0.0)\n",
    "datasci_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}